{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the origin of wine using physiochemical properties\n",
    "\n",
    "by Hina Bandukwala, Yimeng Xia, Sean McKay, Julia Everitt 2023/12/02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle \n",
    "import pandas as pd\n",
    "from myst_nb import glue\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/plain": "0.981"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "accuracy"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "0.982"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "f1"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/html": "<style type=\"text/css\">\n</style>\n<table id=\"T_15b3c\">\n  <thead>\n    <tr>\n      <th id=\"T_15b3c_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n      <th id=\"T_15b3c_level0_col1\" class=\"col_heading level0 col1\" >F1 score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_15b3c_row0_col0\" class=\"data row0 col0\" >0.981000</td>\n      <td id=\"T_15b3c_row0_col1\" class=\"data row0 col1\" >0.982000</td>\n    </tr>\n  </tbody>\n</table>\n",
      "application/papermill.record/text/plain": "<pandas.io.formats.style.Styler at 0x1fb3ec2c850>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "test_scores_df"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Attribution: Code adapted from \n",
    "#https://github.com/ttimbers/breast_cancer_predictor_py/blob/main/report/\n",
    "#breast_cancer_predictor_report.ipynb\n",
    "\n",
    "test_scores_df = pd.read_csv(\"../results/tables/test_results.csv\")\n",
    "glue(\"accuracy\", test_scores_df['accuracy'].values[0], display=False)\n",
    "glue(\"f1\", test_scores_df['F1 score'].values[0], display=False)\n",
    "test_scores_df = test_scores_df.style.format().hide()\n",
    "glue(\"test_scores_df\", test_scores_df, display=False)"
   ]
  },
  {
   "cell_type": "code",
  class_imbalance
   "execution_count": 3,

   "execution_count": 27,
   main
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/plain": "0.1"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "best_C"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "0.9988014800514801"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "train_score"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "0.9834401709401709"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "valid_score"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/html": "<style type=\"text/css\">\n#T_4a00f_row0_col0, #T_4a00f_row0_col2, #T_4a00f_row0_col5, #T_4a00f_row0_col6, #T_4a00f_row0_col11, #T_4a00f_row0_col12, #T_4a00f_row1_col3, #T_4a00f_row1_col7, #T_4a00f_row1_col8, #T_4a00f_row1_col10, #T_4a00f_row2_col1, #T_4a00f_row2_col4, #T_4a00f_row2_col9 {\n  background-color: #b40426;\n  color: #f1f1f1;\n}\n#T_4a00f_row0_col1 {\n  background-color: #f7ba9f;\n  color: #000000;\n}\n#T_4a00f_row0_col3, #T_4a00f_row0_col7, #T_4a00f_row1_col0, #T_4a00f_row1_col1, #T_4a00f_row1_col2, #T_4a00f_row1_col4, #T_4a00f_row1_col9, #T_4a00f_row1_col12, #T_4a00f_row2_col5, #T_4a00f_row2_col6, #T_4a00f_row2_col8, #T_4a00f_row2_col10, #T_4a00f_row2_col11 {\n  background-color: #3b4cc0;\n  color: #f1f1f1;\n}\n#T_4a00f_row0_col4, #T_4a00f_row2_col3 {\n  background-color: #e7745b;\n  color: #f1f1f1;\n}\n#T_4a00f_row0_col8 {\n  background-color: #d44e41;\n  color: #f1f1f1;\n}\n#T_4a00f_row0_col9 {\n  background-color: #f7b99e;\n  color: #000000;\n}\n#T_4a00f_row0_col10 {\n  background-color: #f6a283;\n  color: #000000;\n}\n#T_4a00f_row1_col5 {\n  background-color: #dbdcde;\n  color: #000000;\n}\n#T_4a00f_row1_col6, #T_4a00f_row2_col2 {\n  background-color: #e97a5f;\n  color: #f1f1f1;\n}\n#T_4a00f_row1_col11, #T_4a00f_row2_col0 {\n  background-color: #f4c6af;\n  color: #000000;\n}\n#T_4a00f_row2_col7 {\n  background-color: #e16751;\n  color: #f1f1f1;\n}\n#T_4a00f_row2_col12 {\n  background-color: #d7dce3;\n  color: #000000;\n}\n</style>\n<table id=\"T_4a00f\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_4a00f_level0_col0\" class=\"col_heading level0 col0\" >Alcohol</th>\n      <th id=\"T_4a00f_level0_col1\" class=\"col_heading level0 col1\" >Malicacid</th>\n      <th id=\"T_4a00f_level0_col2\" class=\"col_heading level0 col2\" >Ash</th>\n      <th id=\"T_4a00f_level0_col3\" class=\"col_heading level0 col3\" >Alcalinity_of_ash</th>\n      <th id=\"T_4a00f_level0_col4\" class=\"col_heading level0 col4\" >Magnesium</th>\n      <th id=\"T_4a00f_level0_col5\" class=\"col_heading level0 col5\" >Total_phenols</th>\n      <th id=\"T_4a00f_level0_col6\" class=\"col_heading level0 col6\" >Flavanoids</th>\n      <th id=\"T_4a00f_level0_col7\" class=\"col_heading level0 col7\" >Nonflavanoid_phenols</th>\n      <th id=\"T_4a00f_level0_col8\" class=\"col_heading level0 col8\" >Proanthocyanins</th>\n      <th id=\"T_4a00f_level0_col9\" class=\"col_heading level0 col9\" >Color_intensity</th>\n      <th id=\"T_4a00f_level0_col10\" class=\"col_heading level0 col10\" >Hue</th>\n      <th id=\"T_4a00f_level0_col11\" class=\"col_heading level0 col11\" >0D280_0D315_of_diluted_wines</th>\n      <th id=\"T_4a00f_level0_col12\" class=\"col_heading level0 col12\" >Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_4a00f_level0_row0\" class=\"row_heading level0 row0\" >Class 1</th>\n      <td id=\"T_4a00f_row0_col0\" class=\"data row0 col0\" >0.432437</td>\n      <td id=\"T_4a00f_row0_col1\" class=\"data row0 col1\" >0.036860</td>\n      <td id=\"T_4a00f_row0_col2\" class=\"data row0 col2\" >0.230362</td>\n      <td id=\"T_4a00f_row0_col3\" class=\"data row0 col3\" >-0.318508</td>\n      <td id=\"T_4a00f_row0_col4\" class=\"data row0 col4\" >0.046466</td>\n      <td id=\"T_4a00f_row0_col5\" class=\"data row0 col5\" >0.229783</td>\n      <td id=\"T_4a00f_row0_col6\" class=\"data row0 col6\" >0.286145</td>\n      <td id=\"T_4a00f_row0_col7\" class=\"data row0 col7\" >-0.225637</td>\n      <td id=\"T_4a00f_row0_col8\" class=\"data row0 col8\" >0.097139</td>\n      <td id=\"T_4a00f_row0_col9\" class=\"data row0 col9\" >0.095102</td>\n      <td id=\"T_4a00f_row0_col10\" class=\"data row0 col10\" >0.108393</td>\n      <td id=\"T_4a00f_row0_col11\" class=\"data row0 col11\" >0.336888</td>\n      <td id=\"T_4a00f_row0_col12\" class=\"data row0 col12\" >0.493949</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a00f_level0_row1\" class=\"row_heading level0 row1\" >Class 2</th>\n      <td id=\"T_4a00f_row1_col0\" class=\"data row1 col0\" >-0.508224</td>\n      <td id=\"T_4a00f_row1_col1\" class=\"data row1 col1\" >-0.191436</td>\n      <td id=\"T_4a00f_row1_col2\" class=\"data row1 col2\" >-0.357386</td>\n      <td id=\"T_4a00f_row1_col3\" class=\"data row1 col3\" >0.202615</td>\n      <td id=\"T_4a00f_row1_col4\" class=\"data row1 col4\" >-0.127549</td>\n      <td id=\"T_4a00f_row1_col5\" class=\"data row1 col5\" >-0.002086</td>\n      <td id=\"T_4a00f_row1_col6\" class=\"data row1 col6\" >0.156706</td>\n      <td id=\"T_4a00f_row1_col7\" class=\"data row1 col7\" >0.138050</td>\n      <td id=\"T_4a00f_row1_col8\" class=\"data row1 col8\" >0.129280</td>\n      <td id=\"T_4a00f_row1_col9\" class=\"data row1 col9\" >-0.485420</td>\n      <td id=\"T_4a00f_row1_col10\" class=\"data row1 col10\" >0.300707</td>\n      <td id=\"T_4a00f_row1_col11\" class=\"data row1 col11\" >0.057698</td>\n      <td id=\"T_4a00f_row1_col12\" class=\"data row1 col12\" >-0.480755</td>\n    </tr>\n    <tr>\n      <th id=\"T_4a00f_level0_row2\" class=\"row_heading level0 row2\" >Class 3</th>\n      <td id=\"T_4a00f_row2_col0\" class=\"data row2 col0\" >0.075786</td>\n      <td id=\"T_4a00f_row2_col1\" class=\"data row2 col1\" >0.154576</td>\n      <td id=\"T_4a00f_row2_col2\" class=\"data row2 col2\" >0.127025</td>\n      <td id=\"T_4a00f_row2_col3\" class=\"data row2 col3\" >0.115894</td>\n      <td id=\"T_4a00f_row2_col4\" class=\"data row2 col4\" >0.081083</td>\n      <td id=\"T_4a00f_row2_col5\" class=\"data row2 col5\" >-0.227697</td>\n      <td id=\"T_4a00f_row2_col6\" class=\"data row2 col6\" >-0.442850</td>\n      <td id=\"T_4a00f_row2_col7\" class=\"data row2 col7\" >0.087587</td>\n      <td id=\"T_4a00f_row2_col8\" class=\"data row2 col8\" >-0.226419</td>\n      <td id=\"T_4a00f_row2_col9\" class=\"data row2 col9\" >0.390318</td>\n      <td id=\"T_4a00f_row2_col10\" class=\"data row2 col10\" >-0.409101</td>\n      <td id=\"T_4a00f_row2_col11\" class=\"data row2 col11\" >-0.394586</td>\n      <td id=\"T_4a00f_row2_col12\" class=\"data row2 col12\" >-0.013193</td>\n    </tr>\n  </tbody>\n</table>\n",
      "application/papermill.record/text/plain": "<pandas.io.formats.style.Styler at 0x19288df90>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "coefs"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../results/models/wine_pipeline.pickle', 'rb') as model:\n",
    "    wine_model = pickle.load(model)\n",
    "# grid search results\n",
    "glue(\"best_C\", wine_model.best_params_[\"logisticregression__C\"], display=False)\n",
    "glue(\"train_score\", np.mean(wine_model.cv_results_[\"mean_train_score\"]), display=False)\n",
    "glue(\"valid_score\", np.mean(wine_model.cv_results_[\"mean_test_score\"]), display=False)\n",
    "\n",
    "# coefs\n",
    "coefs_df = pd.DataFrame(data=wine_model.best_estimator_.named_steps['logisticregression'].coef_,\n",
    "                        columns=wine_model.feature_names_in_.tolist(),\n",
    "                        index=['Class 1', 'Class 2', 'Class 3'])\n",
    "\n",
    "glue(\"coefs\", coefs_df.style.background_gradient(cmap='coolwarm'), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this project, we attempted to build a classification model as a proof-of-concept for how logistic regression can be used for classifying wine samples based on their origin using their physiochemical characteristics. We built our classifier using a simple dataset that summarizes 13 physiochemical properties per wine sample along with it's corresponding class based on it's origin/cultivar. Since we are using a \"perfect\" dataset, our final classifer performed very well on the unseen test wine samples with an accuracy score of {glue:text}`accuracy:.2f` and a F1 score of {glue:text}`f1:.2f`. \n",
    "\n",
    "With this project, we intend to showcase that this methodology has potential of streamlining wine identification processes for the benefit of the industry. Since this model is intended as a proof-of-concept, it can be improved significantly by considering other important physiochemical features and modern techniques for feature selection. The model can also be refined with a larger and more complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increased globalization, wine is consumed across a wider range of nations making wine trade an important part of the global economy {cite:p}`Orlandi2015`. For example, in 2021 wine exports increased by an average of 15% since 2017 reaching a global total of $40.7 billion{cite:p}`jain_machine_2023`. Italy is one of the top 5 exporters of wine and together these countries contribute to 70.4% of the total wine exported globally{cite:p}`jain_machine_2023`. As wine consumption becomes integrated into more cultures, there is an increased need for faster and efficient methods for wine certification, identication as well as quality evaluation. Our project focuses on one of those, namely, wine identification. \n",
    "\n",
    "Identification of the wine cultivar (e.g. 'Chardonnay' and 'Merlot') is an important element of consuming and selling wine{cite:p}`ohana-levi_long-term_2023`. Traditional methods rely heavily on the knowledge and experience of indivdual experts which makes the process inherently subjective and labour-intensive. In this project, we aim to use a machine learning algorithm to identify the cultivar of Italian wines using 13 different physiochemical properties instead. This method takes advantage of the dense knowledge-base that exists about the important physiochemical properties of wine. It then utilizes quantitative measurements corresponding to these properties along with machine learning to systematically identify wine cultivars. Given that the wine industry has carved itself a name in global trade, it is crucial to develop and apply cutting-edge methods that can make these processes more accurate, less labour-intensive and cost-efficient. We think that this data-driven approach could be highly beneficial to the wine industry due to the benefits highlighted above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a multivariate dataset for this project that combines 13 physiochemical properties for 178 Italian wine samples. These samples correspond to 3 distinct cultivars from the same geographical location. The data was originally collected by M.Forina et al {cite:p}`forina1998` and contributed to the UC Irvine Machine Learning Repository by Stefan Aeberhard and M. Forina in 1992 (last updated on Aug 28 2023). Details associated with the dataset can be found in the UC Irvine repository (https://archive.ics.uci.edu/dataset/109/wine) and the data can be read directly from here (https://archive.ics.uci.edu/static/public/109/data.csv). Each row of the dataset corresponds to one wine sample and contains measurements corresponding to each of the 13 physiochemical components. Identification and quantification of the different chemical constituents and properties of the wine was based on chromatographic profiles obtained through mass spectrometry{cite:p}`ballabio_classification_2008`. This collection and experimentation was performed by Ballabio, D. et al. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our classification task, we used the logistic regression (LR) algorithm to develop a model that categorizes wine samples into one of three cultivar types based on their origin. These targets can be found in the class column of our dataset. All physiochemical features included in our dataset were used for classification. As a benchmark, we employed scikit-learnâ€™s DummyClassifer as our baseline model which resulted in a 40.33% accuracy with our training dataset. For the LR model, a grid search for the C hyperparameter was performed for values ranging from 0.01 to 1000. The optimal value of {glue:text}`best_C` was used to perform a 10-fold cross-validation and resulted in a accuracy of {glue:text}`train_score:.3f` with our training set and {glue:text}`valid_score:.3f` with our validation set. We primarily used the Python programming language for our analysis. In particular, the following packages were used: NumPy{cite:p}`harris_array_2020`, Pandas{cite:p}`mckinney-proc-scipy-2010`, Altair{cite:p}`VanderPlas2018`, Matplotlib {cite:p}`Hunter:2007`, scikit-learn{cite:p}`scikit-learn`, and ucimlrepo {cite:p}`misc_wine_109`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first separated our dataset into random train and test sets using sklearn's `train_test_split`. The split between train and test sets was 70:30% respectively. We ensures that there would be an equal distribution of target classes across the two sets with the `stratify` argument.  Additionally, a `random_state` was defined to ensure the reproducibility of our results. \n",
    "\n",
    "Please note that the test set was not used to inform any decisions during the analysis except for the final evaluation of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the train set, we first looked at the distribution of values for each numerical feature for each of the three target classes. We can see that the density curves overlap, but still show different shapes and mean values, with some exhibiting bimodal distributions. The least predictive features look to be Magnesium and Ash as there is significant overlap between the 3 class distributions. We decided to keep all of the features to use in our model, as those features may still be more predictive when combined with other features. \n",
    "\n",
    "Next, we looked at the number of training examples belonging to each class to determine if we had a class imbalance, which informs our choice of predictor. The classes seem roughly balanced, with class 2 having the most training examples. Since there is not a significant class imbalance, and the cost of misclassifying 1 vs 2, 2 vs 3 is not very different, the use of accuracy vs other metrics such as F1 is not so important - Accuracy may be enough. Overall, we don't have very many training examples, which may impact the generalizability of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "figure"
    ]
   },
   "source": [
    "![Density plots per class of wine for the 13 physiochemical properties included in the dataset](../results/figures/densities_plot_by_class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: Density plots per class of wine for the 13 physiochemical properties included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distribution plots per class of wine](../results/figures/distribution_by_class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: Distribution plots per class of wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features in the dataset are numerical and take on different ranges. We standardized them using  sklearn's `StandardScaler` with default parameters which does so by removing the mean and scaling to unit variance. Standardization is essential since it prevents features with large variances from dominating the objective function. StandardScaler ensures a consistent scale across features, which aligns with the assumption that logistic regression makes i.e. that features are approximately normally distributed with 0 mean and unit variance. \n",
    "\n",
    "We used sklearn's `ColumnTransformer` to create our preprocesser object that implemented the `StandardScaler`.  The preprocessor was fit on the train set and then used to transform both the train and test sets. \n",
    "\n",
    "The preprocessor object was pickled and can be found in the project directory under results/models `preprocessor_model.pickle`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we built our LR model with sklearn's `LogisticRegression`. Logistic regression was our linear model of choice because it is simple, easy to implement/interpret and can also extend to multiple classes (as is the case with our dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal value of the hyperparameter `C` for our LR model that maximized the accuracy score, we performed a 10-fold cross-validated grid search with sklearn's `GridSearchCV`.\n",
    "The `GridSearchCV` object was constructed using the following parameter grid 0.01, 0.1, 1, 10, 100, 1000 and a pipeline object made with sklearn's `make_pipeline` that implemented `StandardScaler` and `LogisticRegression` with `max_iter` of 2000. The `GridSearchCV` object was fit on the training set. The pickled object can be found here: `results/models/wine_pipeline.pickle`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model was scored on the test set and the resulting `f1` and `accuracy` scores were used to assess model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the model performance and its applicability to wine origin prediction, our current logistic regression model performs quite well on unseen test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accuracy scores for training and validation sets during hyperparameter optimization](../results/figures/wine_cv_C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3: Accuracy scores for training and validation sets during hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} test_scores_df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1: Accuracy and F1 scores after model scoring on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} coefs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2: Feature importances per target class from the LR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve the classification accuracy, we may explore other models such as Support Vector Machines (SVM) and Random Forest to assess if they offer improved test accuracy. \n",
    "In addition, diversifying our evaluation metrics can provide a more comprehensive understanding of our model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Metrics such as precision, recall, F1-score are good choices for imblanced class. According to our baseline model, the accuracy is of 40.33%, indicates that the most prevalent class occurs at a rate of 40%. This suggests a class imbalance (as we have three classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wine-origin-prediction]",
   "language": "python",
   "name": "conda-env-wine-origin-prediction-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
